\chapter{Introduction \& Motivation}
\label{chapter:introduction}
\begin{music}
    \parindent10mm \instrumentnumber{1} \setstaffs1{1} 
    \generalmeter{\meterfrac34} \generalsignature{3}
    \startextract
            \Notes \Dqbu dk \zhl{i*} \en
        \bar
            \Notes \Dqbu hi \zhu{h*} \en
    \zendextract
\end{music}
\epigraph{\textit{one thing that won't change with time is the memories of younger years}}{Minuet of Forest --- Ocarina of Time}

We are living in the wake of milestones in biotechnology and computational biology that suggest reverse-engineering cell biology is within our grasp. The coronavirus pandemic accelerated investment in biotechnology \cite{DeFrancesco2021Financing2020}. The fields of synthetic and systems biology are beginning to resemble engineering disciplines; genetic engineering is becoming more precise, high-throughput single-cell experiments are performed by robots and measurements across all levels of the central dogma are possible: genomics, transcriptomics, proteomics and metabolomics \cite{Perkel2021Single-cellAge}. Advances in micro-fabrication \cite{Shafiee2017TissueMedicine} and in-vitro reconstitutive methods \cite{Gopfrich2018MasteringCells} have allowed biologists to isolate pathways and mechanisms to a level of mathematical and computational tractability \cite{Sharpe2017ComputerTomorrow.}.

The complexity barrier in biology poses a significant challenge. Multiple levels of organisation from the molecular, to cellular, to the whole organism interact with each other, which makes scale separation and accurate approximations difficult. Furthermore, the combinatorial space of possible interactions between macro-molecules involved in the central dogma is intractably large. Systems and synthetic biology have historically made progress through a process of brute force trial and error. This usually involves the interaction of many custom-made parts that are iteratively optimised by human intervention. A trend first observed in the 1980s known as \textit{Eroom's law} revealed that discoveries in biotechnology are becoming slower and more expensive over time, despite improvements in technology \cite{Scannell2012DiagnosingEfficiency}. This problem is exemplified by the declining success rate of clinical trials in the drug discovery process \cite{Wong2019EstimationParameters} and compounded by the ongoing reproducibility crisis \cite{Ioannidis2005WhyFalse.,Mullard2021HalfEffort}. It appears that much of the low-hanging fruit has been picked \cite{Earm2014IntegrativeDevelopment} with methodologies whose standards for transparency, reproducibility and accessibility leave us with much to be desired. 

Despite the widespread lack of mechanistic understanding in human cell biology, sophisticated engineering goals such as targeted modification of the immune system are now possible. In 2018, a T-cell gene therapy --- \emph{tisagenlecleucel} \cite{Halford2021TisagenlecleucelConsiderations} --- for the treatment of adolescent and young adult acute lymphoblastic leukaemia became the most expensive cancer therapy ever, at \$475,000 \cite{Hernandez2018TotalImmunotherapy}. Here, the high cost results from a hugely complex procedure for administering the treatment, which involves harvesting patient T-cells and engineering them to express a novel chimeric antigen receptor (CAR), and then returning the modified T-cells back into the patient. Even in settings where biomanufacturing relies on specific known mechanisms, the manufacturing of viral vectors that deliver the gene therapy is far from straightforward. Human-derived producer cells are used to  synthesise vector \cite{Merten2016ProductionVectors}, but how to engineer them to carry this out efficiently depends on understanding the complexity of these complex cell-types, which involve a great number of unknown mechanisms. To provide clues into the biological processes that limit efficient bioproduction, omics data can be collected along key protocol stages in attempts to understand and optimise production.
\newpage

After a decade of engineering advances in data science and machine learning, epitomised by deep learning methods, theoretical foundations on high-dimensional learning tasks are beginning to condense \cite{Bronstein2021GeometricGauges}. Tasks such as computer vision \cite{mnist2012,imagenet2009}, playing Go \cite{Silver2016}, or protein folding \cite{Jumper2021HighlyAlphaFold} are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: the notion of lower-dimensional \emph{representation}, whereby group equivariant and invariant transformations are composed to capture the appropriate notion of regularity for each task \cite{Bronstein2021GeometricGauges}, and second, learning by local gradient-descent type methods enabled by \emph{differentiable programming} \cite{Innes2019AComputing}.

The emerging picture suggests that bringing together the advances in software and wetware in iterative hypothesis generation and discovery pipelines are key to overcoming the complexity barrier in biology \cite{Sharpe2017ComputerTomorrow.,Ringel2020BreakingLaw,AlQuraishi2021DifferentiableMechanisms}. The term \textit{in silico} has become popularised amongst biologists, which conceptualises a computational model, alongside \emph{in vitro} and \emph{in vivo}, as a method for investigating an organism \emph{in situ}. Researchers are going as far as conceptualising the \emph{digital twin} for personalised medicine \cite{Bjornsson2019DigitalMedicine}. Bringing together deep learning and biomedical research has the potential of importing the notions of \emph{representation} and \emph{differentiability} into biological engineering \cite{AlQuraishi2021DifferentiableMechanisms} and thereby reaping their benefits. Differentiability within engineering pipelines has the same advantage over brute force trial and error as it does over sampling-based algorithms, decreasing the time and cost for biological research. The representations of a study in the form of computational models can increase transparency and reproducibility. Efforts towards open standards have gained traction over the past decade \cite{Malik-Sheriff2020BioModels15Science} with model sharing standards such as Systems Biology Markup Language \cite{Hucka2018TheCore} and data repositories like Human Cell Atlas
\cite{Regev2017TheAtlas} and Flow Repository \cite{Spidlen2012PreparingFlowRepository.org}. The open standards and ethical thresholds of deep learning are also increasing, with the formation of OpenAI \cite{Brockman2016OpenAIGym} and sharing standards such as ONNX \cite{Bai2019ONNX:Exchange}.

\section{Representation of Single Cells}

In this thesis, the focus is on differential equation representations of single cells. In this section we give a brief motivation behind why differential equations are convenient, what other representations exist and how results may translate between differential equations and other representations. When studying a particular biological phenomenon, the appropriate spatio-temporal scale must be chosen that simplifies the mechanism under study while preserving the relationships between experimentally accessible parameters and the resultant behaviours. We do not expect a single unifying tractable model of biological mechanisms under study. In practice there exist multiple equally valid models with various degrees of complexity. For example, the immune system can be described with agent-based models \cite{Chiacchio2014Agent-basedFramework} and also with ordinary differential equations \cite{Bonin2017MathematicalVaccinology}. The model can describe all molecular interactions within a cell \cite{Marucci2020Computer-AidedBiology}, or only the pathways under study \cite{Riccione2012AProcessing}. In cases where tissue organisation or sub-cellular spatial mechanics are important, partial differential equations can be used to characterise them \cite{Sharpe2017ComputerTomorrow.}.
\begin{Figure}
    \includegraphics[width=0.9\linewidth]{figures/top-down-bottom-up}
    \caption{Biological models at different spatial scales and complexities, a large class of which can be cast into differential equation form indicated in bold blue}
    \label{fig:tdbu}
\end{Figure}
Differential equations are convenient because they are usually differentiable, continuous state and continuous time models and hence are already compatible with the two algorithmic principles of deep learning \cite{Bronstein2021GeometricGauges}. In the context of dynamical systems theory it is relatively simple to derive analogous results for discrete state or discrete time models: cellular automata \cite{Martin2017DifferentiableAutomata}, Markov chains \cite{Darling2008DifferentialChains}, stochastic processes and discrete maps just to name a few. A large class of biological models at different spatial scales can be cast into differential equation form (Figure \ref{fig:tdbu}), although this may not always be necessary. With the help of automatic differentiation, programs with arbitrary control flow become piecewise differentiable \cite{Gune2018AutomaticSurvey}. One of the main limitations to consider is the number of branches in the program due to conditional statements and whether there are non-zero derivatives either side of the conditional statement that provide meaningful information. Rule-based models such as agent-based models are examples of programs with many conditional branches often with no meaningful derivatives on either side of the condition. In such scenarios we must fall back to finite difference approaches, requiring a finite number of evaluations of the model. This can be done in a cost-effective way, only evaluating gradients that will likely benefit the optimisation.

Differential equations models span a large range of spatial and temporal scales. It is important to choose a time-scale and space-scale that is relevant to the problem. If one is interested in tissue dynamics, attempting to model DNA conformations within each cell will render the problem intractable. As George Box aptly put \textit{``most models are wrong but some are useful"} so the role of theoretical descriptions in these settings is not necessarily to describe the way reality \textit{is} but serve as tools to bridge the non-intuitive gap between bottom-up and top-down approaches \cite{Gopfrich2018MasteringCells,Powell2018HowScratch,Pezzulo2016Top-downLevel}.
\pagebreak

\subsection{Model Selection \& Reduction}
With such a heterogeneous selection of models that are valid at different spatial and temporal scales it becomes increasingly important to develop tools to navigate the space of models. We want to organise and then choose between different models that explain the same phenomenon whilst also enumerating the set of phenomena that a single model can explain. This problem is known in machine learning as model selection \cite{Ding2018ModelOverview} and establishing relationships between models of varying complexity is know as model reduction \cite{Besselink2013AControl}. We will see in the following section that exploring the space of models inherently involves some form of iterative hypothesis generation workflow.

In the context of parameter inference \cite{Brunton2016SparseSINDYc,GorbachScalableSystems}, sloppiness and sensitivity analysis have been extensively used in the search for reduced models \cite{Daniels2008,Chis2016OnIdentifiability,Gutenkunst2007UniversallyModels,Gabor2017ParameterBiosystems,Villaverde2016StructuralModels}. Linear mappings between models that preserve stoichiometry and reactant structure were investigated \cite{Cardelli2014MorphismsFunction,Cardelli2017MaximalSystems} and computational tools based on partition-refinement were released \cite{Cardelli2017ERODE:Equations}. Structural similarity between reaction networks can be revealed by such mappings, elucidating the functional aspects of complex networks in terms of simpler networks. Nonlinear mappings between models that preserve high-level features without resorting to structural assumptions about the models were still lacking lacking and formed part of the motivation for developing the methods in Chapter \ref{chapter:inference}.

We will see how a suitably defined cost function that focuses on high-level features of a model enables navigation and organisation of models across various levels of complexity. Indeed the focus on higher-level features of differential equation models, such as geometry rather than kinetics, has already been gaining traction in 
pattern formation theory \cite{Halatek2018}.
\pagebreak

\subsection{Phenotyping in Flow Cytometry}
One of the most popular single cell measurement techniques --- routinely used in basic research, clinical practice, and clinical trials --- is flow cytometry. Using this technique, cells expressing potentially many (spectrally distinct) fluorescent reporters can be analysed at the single-cell level. Once a sample containing cells is loaded into the device, the sample fluid is hydrodynamically focused to create a jet of particles aligned so that they pass single file through the interrogation site for fluorescence measurement. Modern cytometers can measure 30+ wavelength channels \cite{McKinnon2018FlowOverview} and even high-dimensional measurements are now possible with the advent of spectral and mass cytometry \cite{Baharlou2019MassStrategies,Nolan2013SpectralCytometry,Cheung2021CurrentSoftware}.
\\
\begin{Figure}
    \includegraphics[width=0.9\linewidth]{flow-cytometry}
    \caption{\textbf{a} A simplified schematic of a flow cytometer illustrating how hydrodynamic focusing results in a single file jet of fluorescent particles whose emission spectra are measured upon excitation with lasers. The forward and side scattered light is focused and unmixed with optical components, preparing the signal for detection and amplification. \textbf{b} The amplified signals are processed with gating strategies in FlowJo or FCSExpress. A typical cell pre-processing strategy include anomalous flow filtering, filtering out clumps from singlets and identifying live cells using zombie dyes.}
    \label{fig:flow-cytometry}
\end{Figure}

Samples are prepared through transfection and expression of fluorescent proteins (as was done in Chapter \ref{chapter:double-exclusive}), staining with fluorescent dyes or fluorescently conjugated antibodies (which was done in Chapter \ref{chapter:exploring}).
Dyes are small polymers designed to bind to amide groups of proteins and have known excitation and emission spectra. Dyes can be bound generally or specifically to proteins depending on the sample preparation \cite{Radjabova2018CharacterisationTARM1}. Zombie dyes, for example, are introduced into the sample to bind to any protein \cite{Perfetto2010AmineReactiveSamples}. As cells die, their plasma membranes become permeable and therefore we expect more intracellular binding of the dye, resulting in a more intense fluorescence signal in dead cells relative to live cells where the dye can only bind to surface proteins. Dyes can be used to specifically target one protein by chemical treatment with a purified mixture of the protein's corresponding antibody. The dye-antibody complex is then introduced into the sample enabling the measurement of the relative concentration of a specific surface proteins across a population of cells. Surface proteins such as CD4 and CD8, for example, distinguish helper and cytotoxic T cell phenotypes. The relative fluorescence intensity signals emitted by the dyes are detected in various wavelengths and amplified to produce raw fluorescence values per particle that passes through the interrogation site.

Immunophenotyping is the analysis of heterogeneous populations of cells involved in the immune response using fluorescent signals. Not all particles that pass through the interrogation site are cells; forward and side scattered light are used to estimate of the size and complexity of each particle and filter out clumps and debris \cite{McKinnon2018FlowOverview}. Scattering is also used to distinguish large cells such as monocytes from smaller ones such as lymphocytes. To account for spectral spill-over between dyes, compensation matrices are applied \cite{McKinnon2018FlowOverview}. Non-linear transforms such as $\mathrm{arcsinh}(x)$ are used to scale the raw fluorescence values, revealing density centres in scatter plots that correspond to different cell phenotypes or activation states.

The process of identifying density centres and annotating cells with domain knowledge is known as \emph{gating}. FlowJo and FCSExpress are, to date, the most popular software tools for analysing cytometry data \cite{Cheung2021CurrentSoftware}. An immunologist would typically create a hierarchy of two dimensional scatter plots --- often referred to as the \emph{gating strategy} --- that includes anomaly filtering, compensation, non-linear transformations and annotation of cell populations. As the dimensionality, throughput, and complexity of cytometry data increases, annotation with manual gating strategies becomes unfeasible. Furthermore, domain experts often disagree on what the gating strategy should be \cite{Overton2019ReportingOntologies}. The demand for user-friendly, interactive analysis tools that leverage high-performance machine learning frameworks is increasing. In Chapter \ref{chapter:exploring} we introduce \emph{FlowAtlas.jl}: an interactive web application that bridges the familiar user-friendly environment of FlowJo and computational tools in Julia developed by the scientific machine learning community.
\section{Differentiability in Biological Engineering}
\subsection{Differentiable Programming}
Differentiable programming an emerging as a paradigm from the widespread use and success of gradient-based optimisation in deep and machine learning \cite{Olah2015NeuralProgramming}. In the early days of neural networks a lot of focus was placed on the back-propagation algorithm \cite{Kelley1960GradientPaths} and its efficient implementation as \emph{backward-passes} in layer modules, enabling the optimisation of millions of parameters in stacks of network-like models. Machine learning frameworks like \emph{TensorFlow} and \emph{PyTorch} were developed to enable the building and sharing of complex models from a set of high-level network primitives such as recurrent, convolutional, dropout and activation layers \cite{MartinAbadi2015Systems,Paszke2019PyTorch:Library}. As machine learning became popular in fields outside of computer science, the desire for domain-specific modifications of the network primitives grew. A program would be considered differentiable if it could be cast into a form compatible with these frameworks which typically involved a substantial increase in the number of unknown parameters. The benefits of restructuring a highly optimised program to make it differentiable via \emph{TensorFlow} and \emph{PyTorch} could be outweighed by the incurred performance issues.
\begin{Figure}
    \includegraphics[width=0.8\linewidth]{automatic-differentiation}
    \caption{Automatic differentiation generates the derivative of a program from its original code, circumventing the need to cast the program into nested mathematical expressions and symbolically evaluating Jacobians.}
    \label{fig:automatic-differentiation}
\end{Figure}
\emph{Automatic differentiation} describes a set of techniques using by machine learning frameworks for evaluating derivatives of programs using low-level primitives \cite{Gune2018AutomaticSurvey}. It is the science of teaching a compiler to apply the chain rule to arbitrary data structures and, crucially, use mathematically exact simplifications to avoid needlessly differentiating elementary operations within the program. This is distinct from symbolic differentiation --- implemented in languages like Mathematica \cite{Cohen2002ComputerAlgorithms} --- as it does not require the explicit computation of intermediate Jacobians. Instead, Jacobian-vector products are computed directly using forward or reverse mode accumulation. The back-propagation algorithm is a specific application of \emph{reverse-mode} accumulation for efficiently computing the derivatives of functions with many more inputs than outputs. \emph{Forward-mode} accumulation is efficient in settings where the number of outputs is much larger than the number of inputs. The problem of efficiently computing derivatives when number of outputs and inputs is comparable is known as \emph{optimal Jacobian accumulation}; an NP-hard problem and an active area of research \cite{Naumann2007OptimalNP-complete}.

The new era of differentiable programming frameworks such as \emph{Flux.jl} and \emph{JAX} enable mixed-mode differentiation of control flow such as loops, branches, recursion, and closures \cite{Innes2018Flux:Julia,Bradbury2018JAX:Programs}. In this setting, highly optimised code no longer needs to be restructured in order to be differentiable. This promises to bring together the high performance computing and machine learning communities to drive forward a new wave innovations such as differentiable physics simulations for robotics \cite{Degrave2019ARobotics}, ray tracing layers in scene understanding \cite{Li2018DifferentiableSampling} and possibly have wide-reaching impacts in biomedicine \cite{AlQuraishi2021DifferentiableMechanisms}. Differentiable programming enables embedding complex existing programs into machine learning pipelines. This not only reduces the parameter space but also takes advantage of the huge amount of knowledge and structure embedded within the existing program. Partial knowledge of mechanisms in biology has given rise to a class of models that have been coined as \emph{grey box models} \cite{Meeds2019EfficientSystems}. In this setting, domain expertise dictates which aspects of the model are highly structured with specific hypotheses and which are composed with generic transformations that obey a chosen set of group symmetries. This blend of traditional, structure-informed, mechanistic modelling with large parameter space machine learning is epitomised by emerging \emph{scientific machine learning} (SciML) community \cite{Rackauckas2020UniversalLearning,Rackauckas2017Differentialequations.jlJulia,Rackauckas2019DiffEqFlux.jlEquations} and positions Julia to displace Python as the \emph{lingua franca} for machine learning \cite{Innes2018OnLanguages}.
\clearpage
\subsection{\emph{Design--Build--Test--Learn} Workflow}
Differentiable programming is relevant to biology in the development of predictive models. Particularly in synthetic biology, where engineering principles are used to the design new biological behaviours. In the effort to move us past brute force trial and error discovery, the paradigm of the \emph{design--build--test--learn} workflow has emerged in synthetic biology \cite{Carbonell2018AnChemicals,Opgenorth2019LessonsLearning}. The workflow imagines the four distinct phases for engineering biological systems. The \emph{design} phase incorporates computational and data-driven approaches to modelling various levels of biological organisation from molecules to organisms. An importance is placed on software tools with the appropriate abstractions that enable tractable investigation of relevant biological questions \cite{Mallavarapu2008ProgrammingBiology,Hucka2018TheCore}. The \emph{build} phase translates between models which exist as software and the real biological system. Similar to how software carries the instructions to be executed on hardware, the \emph{in silico} model should carry the instructions to be executed on \emph{wetware}. Instead of semiconductors, the primitives that facilitate execution are molecular devices and protocols used in molecular and synthetic biology. The biological construct is assembled from a library of DNA components and transfection protocols \cite{ElKaroui2019FutureReport}. The \emph{test} phase involves execution of experiments on the engineered constructs and strains. This phase involves high-throughput methodologies which make use of colony picking robots, flow cytometry, microtitre plates and analytical chemistry for selection of desired phenotypes \cite{Dangi2018CellReview,Shi2011ATherapeutics}. Selected phenotypes can be investigated in more detail with multiomics and assessed at scale with bioreactors \cite{Krassowski2020StateSharing}. This phase generates vast data across various levels of biological organisation. The \emph{learn} phase integrates the various data sources, extracting biological knowledge and compares findings to those generated by models in the design phase. Machine and deep learning methods act in this phase, facilitating the iterative refinement of models in the design phase \cite{Hanczyc2020EngineeringBiology} and generating new protocols and experiments for the build and test phases \cite{Pendleton2019ExperimentManagement,Abate2018ExperimentalSemantics}.

\begin{Figure}
    \includegraphics[width=0.55\linewidth]{figures/dbtl}
    \caption{\emph{design--build--test--learn} cycle from synthetic biology.}
    \label{fig:dbtl}
\end{Figure}
\subsection{Differentiable \emph{Design--Learn} Workflows}
\label{section:design-learn}
In this thesis, the \emph{design--build--test--learn} workflow is applied during the interdisciplinary collaboration described in Chapter \ref{chapter:double-exclusive}. Chapters \ref{chapter:inference}--\ref{chapter:exploring} focused on the \emph{design--learn} part of the workflow can benefit from differentiable programming and tools developed by the scientific machine learning (SciML) community. One of the main lessons from applying this paradigm, as we shall see, is that the workflow must be bidirectional, rather than a cycle, in order for the lab to converge onto their cell design goals efficiently. This way each part of the workflow can operate with its neighbours independently, without being blocked by other parts of the workflow. Let us explore some examples of bi-directional \emph{design--learn} workflows that  can benefit from differentiability.
\begin{Figure}
    \begin{tikzpicture}[node distance=2cm]

        \node (nonparametric) [process, minimum width=5cm] {unstructured inference};
        \node (data) [io, right of=nonparametric, xshift=1cm, yshift=2cm] {observations $\mathcal{D}$};

        \node (field) [decision, below of=nonparametric] {model $F$};
        \node (pred) [io, right of=field,  xshift=2cm] {predictions};

        \draw [arrow] (data) |- (nonparametric);
        \draw [arrow] (nonparametric) -- (field);
        
        \draw [arrow] (field) -- (pred);
        \draw [arrow,color=BurntOrange] (pred) -- +(0,3.5);

    \end{tikzpicture}
    \caption{\emph{Design--learn} workflow without mechanistic knowledge. Predictions generated from the model $F$ are only accurate in the vicinity of data $\targets$}
    \label{fig:experimental-design}
\end{Figure}
Consider time-course gene expression data $\mathcal{D}$, which could be taken via time-lapse microscopy of cells growing on microfluidic plates, optical density measurements from microtiter plate assays or temporal snapshots of flow cytometry measurements. If nothing is known about a mechanism under study, we can infer an unstructured model from a set of observations, and even generate predictions without needing to know anything about the mechanism (Figure \ref{fig:experimental-design}). Unstructured models are too flexible and do not generate accurate predictions outside the input data distribution.

Models $\rates$ constructed with feasible biophysical assumptions have the potential to extrapolate predictions and give concrete biophysical meanings to each parameter $\theta$. This way the experimentalist knows exactly which modification to the system they must make in order to achieve a desired behaviour. More often than not it is also unclear whether the model and its assumptions are reasonable, which brings us to model selection and reduction (Figure \ref{fig:non-parametric}). 
\begin{Figure}
    \begin{tikzpicture}[node distance=2cm]

        \node (field) [decision] {model $F$};
        \node (hypothesis) [io, left of=field,  xshift=-2cm] {hypothesis $\rates$};
        \node (parametric) [process, below of=field, minimum width=5cm] {model selection};
        
        \node (decomp) [process, below of=parametric, minimum width=5cm] {model reduction};
        \node (models) [io, left of=decomp, xshift=-3cm] {models};

        \draw [arrow] (field) -- (parametric);
        \draw [arrow] (hypothesis) |- (parametric);
        \draw [arrow] (parametric) -- (decomp);
        
        \draw [arrow] (decomp) -- (models);
        \draw [arrow,color=ForestGreen] (models) -- +(0,3.5);

    \end{tikzpicture}
    \caption{Overview of model selection, reduction and refinement loop}
    \label{fig:non-parametric}
\end{Figure}
Alternatively one may construct $\rates$ to cover a whole class of models rather than a single model \cite{Kuepfer2007EnsembleDynamics}. The expectation is that most of the parameters would be zero but some would be informative \cite{Brunton2016SparseSINDYc}. From the inferred parameters $\theta$ one may construct alternative hypotheses and narrow down the set of plausible models. By iterating this procedure one would identify the minimal model within the model class that explains the data.

\pagebreak
\section{Thesis Content Summary}
In this thesis we focus on the benefits of differential equation representations and differentiable \emph{design--learn} workflows in the context of the genetic engineering of cell phenotypes in synthetic biology. We will see how a differential equation representation of a cell or population of cells requires an exploration of the relationship between bifurcation theory and the concept of a phenotype. In Chapter \ref{chapter:background} we introduce the reader a relevant background in differential equations and machine learning with applications in cell biology to set the stage for the following chapters:  chapters \ref{chapter:double-exclusive}--\ref{chapter:inference} contain two \emph{incorporated publications} and chapter \ref{chapter:exploring} is an adaptation of work that has not been published at the time of writing the thesis. All published work is embedded within a black bounding box for clarity to the reader. Chapter \ref{chapter:double-exclusive} exhibits the results of an interdisciplinary collaboration in synthetic biology, during which an iterative \emph{design--learn} workflow was followed in an attempt to overcome the biological complexity barrier. During this collaboration, a disconnect between design goals and parameter inference methods was identified which lead to the novel bifurcation inference method published in Chapter \ref{chapter:inference}. Concurrently, a focus on flow cytometry during the collaboration in Chapter 
\ref{chapter:double-exclusive} lead to the development of an interactive tool \emph{FlowAtlas.jl} in Chapter \ref{chapter:exploring}. We explore the importance of interactive exploration of high-dimensional flow cytometry data for immunophenotyping. Finally, Chapter \ref{chapter:conclusions} concludes with retrospectives on the previous chapters, which includes a revised view of a \emph{design--learn} workflow for synthetic biology, that in principle is completely differentiable. Furthermore, we propose how one would use concepts from Chapters \ref{chapter:inference}--\ref{chapter:exploring} to interactivity explore the space of hypotheses that represent the same cell phenotype. Our conclusions would be most impactful in cell line development where flow cytometry and other omic-type measurements are taken at different protocol stages of a biomanufacturing process.